{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aupd3BGix5HK"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smk6cfsDdQbO"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This colab analyzes the [GREAT](https://great.stanford.edu) results for\n",
        "functional enrichment of the input GWAS results.\n",
        "\n",
        "It assumes you have already submitted the input GWAS results to the GREAT server\n",
        "using the web API. This is performed in the following way:\n",
        "\n",
        "## Converting an `association_results.loci.tsv` file to a BED file\n",
        "\n",
        "Given the GWAS input file `association_results.loci.tsv`, convert to BED format\n",
        "file named `association_results.bed` with the following command:\n",
        "\n",
        "```bash\n",
        "tail -n+2 association_results.loci.tsv \\\n",
        "  awk -F'\\t' -vOFS='\\t' '{print $2, $3-1, $3, NR}' \\\n",
        "  \u003e association_results.bed\n",
        "```\n",
        "\n",
        "## Running GREAT web API on the resulting BED.\n",
        "\n",
        "Assuming that your BED file is accessible at\n",
        "\n",
        "https://my.domain.com/association_results.bed\n",
        "\n",
        "run the web API and write to the file `association_results.great` in the\n",
        "following way:\n",
        "\n",
        "```bash\n",
        "# Run the GREAT command and write to a temporary file.\n",
        "# See https://great-help.atlassian.net/wiki/spaces/GREAT/pages/655447/Programming+Interface for details.\n",
        "wget -O association_results.great.tmp \\\n",
        "  http://bejerano.stanford.edu/great/public/cgi-bin/greatStart.php?outputType=batch\u0026requestSpecies=hg19\u0026requestName=myjob\u0026requestSender=Client+A\u0026requestURL=https%3A%2F%2Fmy.domain.com%2Fassociation_results.bed\n",
        "\n",
        "# Strip off some bad formatting from GREAT for the final output file.\n",
        "awk '{if(index($0, \"\u003cscript\u003e\") == 1) {print substr($0, index($0, \"\u003c/script\u003e\")+9) } else {print $0}}' association_results.great.tmp \u003e association_results.great\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSpdr5btfVVF"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from typing import Optional, Sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYDBYduD0pOn"
      },
      "source": [
        "## Running analyses once the GREAT TSV files are generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf0N1vm0h1Da"
      },
      "outputs": [],
      "source": [
        "ASSEMBLY = 'hg19'\n",
        "\n",
        "# All columns in a single ontology output file from https://great.stanford.edu\n",
        "ALL_COLUMNS = [\n",
        "    'Ontology',\n",
        "    'ID',\n",
        "    'Desc',\n",
        "    'BinomRank',\n",
        "    'BinomP',\n",
        "    'BinomBonfP',\n",
        "    'BinomFdrQ',\n",
        "    'RegionFoldEnrich',\n",
        "    'ExpRegions',\n",
        "    'ObsRegions',\n",
        "    'GenomeFrac',\n",
        "    'SetCov',\n",
        "    'HyperRank',\n",
        "    'HyperP',\n",
        "    'HyperBonfP',\n",
        "    'HyperFdrQ',\n",
        "    'GeneFoldEnrich',\n",
        "    'ExpGenes',\n",
        "    'ObsGenes',\n",
        "    'TotalGenes',\n",
        "    'GeneSetCov',\n",
        "    'TermCov',\n",
        "    'Regions',\n",
        "    'Genes',\n",
        "]\n",
        "\n",
        "# The columns that are related to significance testing.\n",
        "SIGNIFICANCE_COLUMNS = [\n",
        "    'BinomP',\n",
        "    'BinomBonfP',\n",
        "    'BinomFdrQ',\n",
        "    'HyperP',\n",
        "    'HyperBonfP',\n",
        "    'HyperFdrQ',\n",
        "]\n",
        "\n",
        "# The subset of columns we typically care to compare between two analyses.\n",
        "JOINED_COLUMN_PREFIXES = (\n",
        "    ['Desc'] + SIGNIFICANCE_COLUMNS + ['RegionFoldEnrich', 'GeneFoldEnrich']\n",
        ")\n",
        "\n",
        "# These are the default ontologies we keep. It excludes \"Ensembl Genes\" and\n",
        "# \"Mouse Phenotype\" ontologies, the former because enrichment for a single gene\n",
        "# is not relevant when analyzing GWAS results and the latter because empirically\n",
        "# we have found the single knockout version of the Mouse Phenotype ontology to\n",
        "# provide cleaner enrichment signals.\n",
        "_DEFAULT_ONTOLOGIES = (\n",
        "    'GO Biological Process',\n",
        "    'GO Cellular Component',\n",
        "    'GO Molecular Function',\n",
        "    'Human Phenotype',\n",
        "    'Mouse Phenotype Single KO',\n",
        ")\n",
        "\n",
        "\n",
        "def load_great_results(\n",
        "    great_tsv: str, ontologies: Optional[Sequence[str]] = _DEFAULT_ONTOLOGIES\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Returns a pd.DataFrame of the given dataset.\n",
        "\n",
        "  This assumes the TSV passed in is of the format returned by the GREAT\n",
        "  programmatic interface, which has all ontology-specific results concatenated\n",
        "  into a single file.\n",
        "\n",
        "  Args:\n",
        "    great_tsv: The input TSV file returned by running GREAT on the data of\n",
        "      interest.\n",
        "    ontologies: An optional list of ontologies to restrict results to. If\n",
        "      unspecified, the default ontologies are retained. If an empty sequence or\n",
        "      None is passed in, all ontologies are retained.\n",
        "\n",
        "  Returns:\n",
        "    A DataFrame of the results.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: The column headers are not ordered as expected.\n",
        "  \"\"\"\n",
        "  with open(great_tsv) as f:\n",
        "    lines = f.readlines()\n",
        "  comment_lines = [line for line in lines if line.startswith('#')]\n",
        "  if comment_lines[3][2:-1].split() != ALL_COLUMNS:\n",
        "    raise ValueError('Column headers not ordered as expected')\n",
        "  if (\n",
        "      comment_lines[0]\n",
        "      != f'# GREAT version 4.0.4\\tSpecies assembly: hg19\\tAssociation rule: '\n",
        "      f'Basal+extension: 5000 bp upstream, 1000 bp downstream, 1000000 bp max '\n",
        "      f'extension, curated regulatory domains included\\n'\n",
        "  ):\n",
        "    raise ValueError(f'Unexpected initial comment line: {comment_lines[0]}')\n",
        "\n",
        "  df = pd.read_csv(\n",
        "      io.StringIO(''.join(lines)), sep='\\t', names=ALL_COLUMNS, comment='#'\n",
        "  )\n",
        "\n",
        "  if ontologies:\n",
        "    keep_mask = df.Ontology.isin(ontologies)\n",
        "    print(\n",
        "        f'Retaining {keep_mask.sum()} of {len(df)} entries in {great_tsv} '\n",
        "        f'when restricting to ontologies {ontologies}'\n",
        "    )\n",
        "    df = df[keep_mask]\n",
        "  return df.set_index(['Ontology', 'ID'])\n",
        "\n",
        "\n",
        "def significant_results(\n",
        "    df: pd.DataFrame,\n",
        "    columns: Sequence[str],\n",
        "    threshold: float,\n",
        "    description_filter: str = '',\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Returns a copy of the subset of `df` that satisfy the threshold.\"\"\"\n",
        "  masks = [df[col] \u003c= threshold for col in columns]\n",
        "  if description_filter:\n",
        "    masks.append(df.Desc.str.contains(description_filter, case=False))\n",
        "  mask = np.logical_and.reduce(masks)\n",
        "  return df[mask].copy()\n",
        "\n",
        "\n",
        "def create_latex_table(df: pd.DataFrame) -\u003e str:\n",
        "  cardio_respiratory_df = significant_results(\n",
        "      df,\n",
        "      ['BinomBonfP', 'HyperBonfP'],\n",
        "      threshold=0.05,\n",
        "      description_filter=(\n",
        "          r'cardiac|cardio|cardial|heart|circulatory|respir|lung|pulmon'\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  original_precision = pd.get_option('display.precision')\n",
        "  pd.set_option('display.precision', 2)\n",
        "  columns = ['ID', 'Desc', 'BinomBonfP', 'HyperBonfP', 'ObsRegions']\n",
        "  name_map = {\n",
        "      'ID': 'Term',\n",
        "      'Desc': 'Description',\n",
        "      'BinomBonfP': 'Region P',\n",
        "      'HyperBonfP': 'Gene P',\n",
        "      'ObsRegions': 'Num regions',\n",
        "  }\n",
        "  retval = (\n",
        "      cardio_respiratory_df.reset_index()[columns]\n",
        "      .sort_values('BinomBonfP')\n",
        "      .rename(columns=name_map)\n",
        "      .to_latex(index=False)\n",
        "  )\n",
        "  pd.set_option('display.precision', original_precision)\n",
        "  return (\n",
        "      r\"\"\"\\begin{table}[ht]\n",
        "\\small\n",
        "\\centering\n",
        "\"\"\"\n",
        "      + retval\n",
        "      + r\"\"\"\\caption{\\textbf{Cardiovascular and respiratory term enrichments of the ML-based\n",
        "COPD loci.} Enrichments were computed using GREAT~\\cite{McLean2010} with default\n",
        "parameters. The 82 total terms significant at Bonferroni-corrected P-value 0.05\n",
        "by both the region-based binomial and gene-based hypergeometric tests were\n",
        "filtered to those with description that matched the regular expression\n",
        "`cardiac|cardio|cardial|heart|circulatory|respir|lung|pulmon'.}\n",
        "\\end{table}\n",
        "\"\"\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnHeXWmgNApN"
      },
      "source": [
        "# Compute enrichments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp0AELX0NADJ"
      },
      "outputs": [],
      "source": [
        "# The path to the enrichments for the GWAS.\n",
        "g_great_path = ''  # @param\n",
        "\n",
        "# Load results.\n",
        "g_df = load_great_results(g_great_path)\n",
        "\n",
        "# Write the LaTeX table of results of respiratory/cardio terms.\n",
        "print(create_latex_table(g_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imOsf0JzSnKc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/genomics/internal:genomics_colab",
        "kind": "private"
      },
      "name": "GREAT_analyses.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/learning/genomics/medgen/phenotyping/spirometry/notebooks/GREAT_analysis_of_spirometry_loci.ipynb",
          "timestamp": 1656082226303
        },
        {
          "file_id": "/piper/depot/google3/learning/genomics/medgen/phenotyping/spirometry/notebooks/GREAT_analysis_of_spirometry_loci.ipynb",
          "timestamp": 1646366708138
        },
        {
          "file_id": "/piper/depot/google3/learning/genomics/medgen/colab/notebooks/spirometry/GREAT_analysis_of_spirometry_loci.ipynb",
          "timestamp": 1636592786246
        },
        {
          "file_id": "1AN5nFtLpiWyMCX_7DG1IZGmjI-wq8P9o",
          "timestamp": 1629464979515
        },
        {
          "file_id": "/piper/depot/google3/learning/genomics/medgen/colab/notebooks/spirometry/GREAT_analysis_of_spirometry_loci.ipynb",
          "timestamp": 1627996241493
        },
        {
          "file_id": "1sqrdwk4hEwf8s-on79JjzYAtHrbHRB12",
          "timestamp": 1623205862231
        },
        {
          "file_id": "10O9SxPmVt_054xweK1KmcD9_qZMZyHlH",
          "timestamp": 1598296250724
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
