{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEOKiQF6EudS"
      },
      "source": [
        " Copyright 2023 Google LLC.\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nV1zyipOE66"
      },
      "source": [
        "#bbVIPRS Implementation Example\n",
        "\n",
        "### Author: nickfurlotte@google.com\n",
        "### Date: \u003cINSERT DATE\u003e\n",
        "\n",
        "In this colab, we provide an example implementation of black box variational\n",
        "inference for PRS (bbviPRS)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbqZGJTR6Y2w"
      },
      "source": [
        "## Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXBr6EJf6bdq"
      },
      "outputs": [],
      "source": [
        "# Whether to use TPUs in the colab. If False, then will default to CPU.\n",
        "USE_TPU = True\n",
        "# The p-value threshold bbviprs uses to select SNPs from the sumstats results.\n",
        "BBVIPRS_PVALUE_THRESH = 1e-2\n",
        "# How many optimization steps to take.\n",
        "BBVIPRS_OPTIMIZATION_STEPS = 200\n",
        "# Learning rate for the bbviprs optimization.\n",
        "BBVIPRS_LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk0cqtIhRuj2"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxZf-T_ARwwe"
      },
      "outputs": [],
      "source": [
        "!pip install bed-reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHgrh9jhRxob"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TkrQ8kNRyr5"
      },
      "outputs": [],
      "source": [
        "import contextlib \n",
        "from typing import Callable, Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bed_reader\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP7kxaC8xdti"
      },
      "source": [
        "## Initialize TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KSh4-Wpxcst"
      },
      "outputs": [],
      "source": [
        "if USE_TPU:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  # This is the TPU initialization code that has to be at the beginning.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print('All devices: ', tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eSnbsG6OHrJ"
      },
      "source": [
        "## Download input files used in LDPred2 tutorial\n",
        "\n",
        "Find the [tutorial here](https://privefl.github.io/bigsnpr/articles/LDpred2.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCaK-NiMN-9K"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/privefl/bigsnpr/raw/master/data-raw/public-data3.zip\n",
        "!unzip public-data3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694QTvYATQgX"
      },
      "source": [
        "## Process data for analysis\n",
        "\n",
        "We follow processing steps similar to the LDPred2 tutorial referenced above.\n",
        "This essentially means that we harmonize the data between the sumstats\n",
        "and the genotype data by matching the SNP sets and making sure that\n",
        "SNPs and effect sizes match in direction. In addition, we perform a simple\n",
        "Z-score normalization on the SNP matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQi0ZAAZT_LN"
      },
      "source": [
        "### Read bed and grab SNP matrix and phenotype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TQ_L5PqOPmv"
      },
      "outputs": [],
      "source": [
        "bed_file = bed_reader.open_bed('tmp-data/public-data3.bed')\n",
        "snp_matrix = np.array(bed_file.read())\n",
        "pheno = bed_file.pheno.astype(float)\n",
        "print(snp_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR7OlkizUBpB"
      },
      "source": [
        "### Read sumstat file and reorder data so that SNPs match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdMEHqiDRodw"
      },
      "outputs": [],
      "source": [
        "sumstats = pd.read_csv('tmp-data/public-data3-sumstats.txt')\n",
        "sumstats.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWmhNn7KTeug"
      },
      "outputs": [],
      "source": [
        "common_rsid = np.intersect1d(bed_file.sid, sumstats.rsid)\n",
        "sumstat_common = sumstats.set_index('rsid').loc[common_rsid].reset_index()\n",
        "snp_matrix_common = pd.DataFrame(snp_matrix, columns=bed_file.sid)[\n",
        "    common_rsid\n",
        "].values\n",
        "\n",
        "assert sumstat_common.shape[0] == snp_matrix_common.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-m9iv6cZCN4"
      },
      "source": [
        "### Check and fix SNP direction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu_YLzHIZBnk"
      },
      "outputs": [],
      "source": [
        "# Note: In BED files the het for the first allele is encoded as 1.\n",
        "bed_snp_order = (\n",
        "    pd.DataFrame(\n",
        "        [bed_file.allele_1, bed_file.allele_2],\n",
        "        columns=bed_file.sid,\n",
        "        index=['a1', 'a0'],\n",
        "    ).T.loc[common_rsid]\n",
        ")[['a0', 'a1']]\n",
        "sumstat_snp_order = sumstat_common[['rsid', 'a0', 'a1']].set_index('rsid')\n",
        "\n",
        "# Sanity check that the rows are in the same order.\n",
        "assert np.all(bed_snp_order.index == sumstat_snp_order.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYBdORNdgPkA"
      },
      "source": [
        "### Check that we see the same numbers as reported by the LDPred2 tutorial.\n",
        "\n",
        "\"45,337 variants have been matched; 22,758 were flipped and 15,092 were reversed.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTocmZ50doqe"
      },
      "outputs": [],
      "source": [
        "_FLIP_MAP = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n",
        "\n",
        "\n",
        "def check_snps():\n",
        "  \"\"\"Evaluate the two SNP encodings to decide if they need to be adjusted.\"\"\"\n",
        "  flip_count = 0\n",
        "  reverse_count = 0\n",
        "  reverse_rsids = []\n",
        "  for (rsid, sumstat), (_, bed) in zip(\n",
        "      sumstat_snp_order.iterrows(), bed_snp_order.iterrows()\n",
        "  ):\n",
        "    if not len(np.intersect1d(sumstat.values, bed.values)):\n",
        "      flip_count += 1\n",
        "      bed.a0 = _FLIP_MAP[bed.a0]\n",
        "      bed.a1 = _FLIP_MAP[bed.a1]\n",
        "    if (sumstat.values == bed.values).sum() != 2:\n",
        "      reverse_count += 1\n",
        "      reverse_rsids.append(rsid)\n",
        "  return flip_count, reverse_count, reverse_rsids\n",
        "\n",
        "\n",
        "flip_count, reverse_count, reverse_rsids = check_snps()\n",
        "\n",
        "print(f'Flip count: {flip_count}, Reverse count: {reverse_count}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAE4qs1EhbfW"
      },
      "source": [
        "Cool those numbers match. Flipping doesn't change the direction of effect\n",
        "only reversing does, so we kept up with the SNPs that need to be reversed.\n",
        "Then we simply change the direction of their effects in the sumstats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbcyHAK2izeX"
      },
      "outputs": [],
      "source": [
        "reverse_mask = np.in1d(sumstat_common.rsid, reverse_rsids)\n",
        "sumstat_common = sumstat_common.assign(\n",
        "    beta=lambda d: np.where(reverse_mask, -d.beta, d.beta)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqrJWedVFZB"
      },
      "source": [
        "### Perform simple normalization on SNP matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxcq5QASVH-s"
      },
      "outputs": [],
      "source": [
        "snp_mean, snp_sd = (\n",
        "    snp_matrix_common.mean(axis=0),\n",
        "    np.std(snp_matrix_common, axis=0),\n",
        ")\n",
        "snps = (snp_matrix_common - snp_mean) / snp_sd\n",
        "\n",
        "# Should look like 0,1 or close.\n",
        "snps.mean(axis=0).mean(), snps.var(axis=1).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "annbD5qxl-dc"
      },
      "source": [
        "Usually you would also do QC but not required here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A38BUoUnkmoU"
      },
      "source": [
        "## Prep data for running bbviPRS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aN8kQXaxAnl"
      },
      "source": [
        "### For bbviPRS-select we will set a p-value threshold and only fit the model over those SNPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnNYSdtXwhC7"
      },
      "outputs": [],
      "source": [
        "snp_mask = sumstat_common.p \u003c= BBVIPRS_PVALUE_THRESH\n",
        "num_snps = snp_mask.sum()\n",
        "print(f'Total number of SNPs selected for VI: {num_snps}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u_hx_AyxH3p"
      },
      "outputs": [],
      "source": [
        "snp_betas = sumstat_common.beta[snp_mask]\n",
        "snp_betas_se = sumstat_common.beta_se[snp_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-VWCR6UxeGh"
      },
      "outputs": [],
      "source": [
        "snp_matrix = snps[:, snp_mask]\n",
        "print(snp_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLS0ePpkxox5"
      },
      "source": [
        "### Compute LD matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPTZsqncxoHK"
      },
      "outputs": [],
      "source": [
        "ld_matrix = np.corrcoef(snp_matrix.T)\n",
        "print(ld_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymNRyiLx1la"
      },
      "source": [
        "## bbviPRS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VSB9JiWJ552"
      },
      "source": [
        "### A little setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqtikVEGJ0gD"
      },
      "outputs": [],
      "source": [
        "betas_normed = snp_betas / snp_betas_se\n",
        "gwas_betas = tf.convert_to_tensor(betas_normed, dtype=tf.float32)\n",
        "ld_matrix_tensor = tf.convert_to_tensor(ld_matrix, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0NdBF2caZ9r"
      },
      "source": [
        "### Define our prior and posterior functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsjSkMTdKJtk"
      },
      "outputs": [],
      "source": [
        "def get_vi_mixture_prior(\n",
        "    size: int, scale1: float, scale2: float, mixture_prob: float\n",
        ") -\u003e tf.keras.Model:\n",
        "  \"\"\"Create a mixture of normals prior with two components.\n",
        "\n",
        "  Args:\n",
        "    size: The number of SNP effect sizes.\n",
        "    scale1: The standard deviation of the first normal component.\n",
        "    scale2: The standard deviation of the second normal component.\n",
        "    mixture_prob: The probability that an effect comes from distribution one.\n",
        "\n",
        "  Returns:\n",
        "    Returns a keras model.\n",
        "  \"\"\"\n",
        "  num_components = 2\n",
        "  probs = np.array(\n",
        "      [[mixture_prob, 1.0 - mixture_prob] for i in range(size)]\n",
        "  ).astype(np.float32)\n",
        "  locs = np.array([np.zeros((num_components,)) for i in range(size)]).astype(\n",
        "      np.float32\n",
        "  )\n",
        "  scales = np.array([[scale1, scale2] for i in range(size)]).astype(np.float32)\n",
        "\n",
        "  def build_distribution(_):\n",
        "    return tfd.Independent(\n",
        "        tfd.MixtureSameFamily(\n",
        "            mixture_distribution=tfd.Categorical(probs=probs),\n",
        "            components_distribution=tfd.Normal(loc=locs, scale=scales),\n",
        "        ),\n",
        "        reinterpreted_batch_ndims=1,\n",
        "    )\n",
        "\n",
        "  return tf.keras.Sequential(\n",
        "      [tfp.layers.DistributionLambda(build_distribution)]\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvl5GFL8LFgA"
      },
      "source": [
        "The function above returns a `keras` model, which when called returns a distribution object. The distribution in this case is a mixture where\n",
        "`mixture_prob` of the effects come from a Normal with mean zero and variance, `scale_1`, while the `1-mixture_prob` variants come from a Normal with\n",
        "mean zero and variance `scale_2`. We can use this to mimic the behavior of\n",
        "the standard LDPred mixture prior. Here is an example.\n",
        "\n",
        "It is a little strange to return a model that returns a distribution, but we do this to make the larger model building easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKeuCaZ_KfMY"
      },
      "outputs": [],
      "source": [
        "prior_model = get_vi_mixture_prior(num_snps, 0.5, 1e-7, 0.20)\n",
        "sample_beta = prior_model(0).sample()\n",
        "\n",
        "_ = plt.figure(figsize=(10, 8))\n",
        "_ = plt.hist(sample_beta.numpy(), 100)\n",
        "_ = plt.title(\n",
        "    'Sample from a mixture prior where 80% of the effects are '\n",
        "    'clustered around zero'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CB7bnkDAibU"
      },
      "source": [
        "In the above plot, you can see that the majority of effects are effectively zero,\n",
        "while the minority is spread far away from zero. As a result of this\n",
        "induced distribution, the prior will enforce sparsity of non-zero effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66gw2wjrylWO"
      },
      "outputs": [],
      "source": [
        "def get_vi_posterior(\n",
        "    size: int, beta_init: Optional[tf.Tensor] = None\n",
        ") -\u003e tf.keras.Model:\n",
        "  \"\"\"Create the $N(m, K)$ posterior used for VI inference.\n",
        "\n",
        "  Args:\n",
        "    size: The number of SNP effect sizes.\n",
        "\n",
        "  Returns:\n",
        "    Returns a keras model.\n",
        "  \"\"\"\n",
        "  if beta_init is None:\n",
        "    beta_init = tf.zeros(size)\n",
        "\n",
        "  def build_distribution(t):\n",
        "    return tfd.Independent(\n",
        "        tfd.Normal(loc=t[..., 0], scale=tf.math.softplus(t[..., 1])),\n",
        "        reinterpreted_batch_ndims=1,\n",
        "    )\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(\n",
        "          shape=[size, 2],\n",
        "          dtype=tf.float32,\n",
        "          initializer=tfp.layers.BlockwiseInitializer(\n",
        "              [\n",
        "                  tf.keras.initializers.Constant(beta_init),\n",
        "                  tf.keras.initializers.Constant(np.log(np.expm1(1.0))),\n",
        "              ],\n",
        "              sizes=[1, 1],\n",
        "          ),\n",
        "      ),\n",
        "      tfp.layers.DistributionLambda(build_distribution),\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UNZUuQ2MeuV"
      },
      "outputs": [],
      "source": [
        "posterior_model = get_vi_posterior(num_snps)\n",
        "sample_beta = posterior_model(0).sample()\n",
        "\n",
        "_ = plt.figure(figsize=(10, 8))\n",
        "_ = plt.hist(sample_beta.numpy(), 100)\n",
        "_ = plt.title('Sample from an initial surrogate posterior that is Normal(m, V)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYMqGmkrN_eq"
      },
      "source": [
        "### Create the model for optimization.\n",
        "\n",
        "We first create a `keras.Layer` to compute the loss function and then we wrap\n",
        "that inside of a larger `keras.Model` object. Again, this seems a little odd,\n",
        "but this setup makes it easier to expand the model later to incorporate more\n",
        "complexities (such as additional data sources)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raP0b_goRGls"
      },
      "outputs": [],
      "source": [
        "class ViLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      size: int,\n",
        "      prior_mixture: float,\n",
        "      prior_scale_1: float,\n",
        "      prior_scale_2: float,\n",
        "      beta_init: Optional[tf.Tensor] = None,\n",
        "  ):\n",
        "    super(ViLayer, self).__init__()\n",
        "    self.prior_model = get_vi_mixture_prior(\n",
        "        size, prior_scale_1, prior_scale_2, prior_mixture\n",
        "    )\n",
        "    self.posterior_model = get_vi_posterior(size, beta_init)\n",
        "    self.log_prob_norm = tf.convert_to_tensor(1.0 / size, dtype=tf.float32)\n",
        "    self.size = size\n",
        "\n",
        "  def loss_fn(self):\n",
        "    \"\"\"Implements a mean squared error loss with a VI objective.\n",
        "\n",
        "    We use a generative model approach to optimize the posterior distribution.\n",
        "    Assume that $\\beta$ is sampled from Prior(...) and then $\\beta_tilde$ is\n",
        "    computed\n",
        "    as $\\beta_tilde = tf.dot(LD_Matrix, \\beta)$. To optimize the posterior,\n",
        "    we generate a $\\beta_tilde$ and then compute the loss as\n",
        "    $MSE(\\beta_tilde, \\beta_from_gwas) + $ Variational Inference Objective,\n",
        "    where the VI objective is the KL divergence between the prior and posterior.\n",
        "    \"\"\"\n",
        "    prior = self.prior_model(0)\n",
        "    posterior = self.posterior_model(0)\n",
        "    beta_sample = posterior.sample()\n",
        "    beta_transform = tf.tensordot(ld_matrix_tensor, beta_sample, 1)\n",
        "    kl_samples = posterior.sample(_NUM_KL_APPROX_SAMPLES)\n",
        "    log_prob_norm = tf.convert_to_tensor(1.0 / self.size, dtype=tf.float32)\n",
        "    loss = tf.divide(\n",
        "        tf.reduce_sum(tf.square(tf.subtract(gwas_betas, beta_transform))),\n",
        "        self.size,\n",
        "    )\n",
        "    return loss + tf.multiply(\n",
        "        _KL_WEIGHT,\n",
        "        tf.multiply(\n",
        "            self.log_prob_norm,\n",
        "            tf.reduce_mean(\n",
        "                posterior.log_prob(kl_samples) - prior.log_prob(kl_samples)\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    self.add_loss(self.loss_fn())\n",
        "    return self.posterior_model(0).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbbkBsdkbTAw"
      },
      "source": [
        "We found it useful to initialize the posterior mean with a reasonable value.\n",
        "One obvious choice is the infinitesimal model solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_w3dE3qcQeL"
      },
      "outputs": [],
      "source": [
        "def compute_inf_model_posterior(\n",
        "    ld_matrix: tf.Tensor, beta: tf.Tensor, scale: float, diag: bool = False\n",
        ") -\u003e Tuple[tf.Tensor, tf.Tensor]:\n",
        "  r\"\"\"Compute mean and variance for the infinitesimal model posterior.\n",
        "\n",
        "  Given a GWAS effect size vector beta and assuming a normal prior on the\n",
        "  true underlying effect size ($N(0, scale**2)$), we can compute the posterior\n",
        "  P(SNP effect sizes | GWAS effect sizes) as $N(\\mu, \\Sigma)$, where\n",
        "  $\\mu = \\beta$ and $\\Sigma = (LDMatrix + I*1/scale**2)^{-1}$.\n",
        "\n",
        "  Args:\n",
        "    ld_matrix: The NxN matrix of SNP correlations.\n",
        "    beta: A vector of length N representing the GWAS effects.\n",
        "    scale: The stddev of the prior distribution - N(0, scale**2).\n",
        "    diag: Whether the LDMatrix is a diagonal matrix.\n",
        "\n",
        "  Returns:\n",
        "    The mean and variance of the analytical posterior.  The mean is a tensor\n",
        "    of shape [N], and the variance is of shape [N, N].\n",
        "  \"\"\"\n",
        "  size = ld_matrix.shape[0]\n",
        "  # Sometimes beta is of shape [N], sometimes it is of shape [N, 1].\n",
        "  beta = tf.squeeze(beta)\n",
        "  if diag:\n",
        "    post_var = tf.convert_to_tensor(\n",
        "        1.0 / (1.0 + 1 / scale**2), dtype=tf.float32\n",
        "    )\n",
        "    post_mean = post_var * beta\n",
        "  else:\n",
        "    post_var = tf.linalg.inv(\n",
        "        (ld_matrix + tf.linalg.diag(tf.ones(size) * 1 / scale**2))\n",
        "    )\n",
        "    post_mean = tf.linalg.matvec(post_var, beta)\n",
        "  return post_mean, post_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvAAh0Rdgq3U"
      },
      "source": [
        "Since our dataset is small we can easily look at a range of shrinkage parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4d5vWrgcqCm"
      },
      "outputs": [],
      "source": [
        "sumstat_samples_size = sumstat_common['N'][0]\n",
        "\n",
        "for h in [1e-2, 1e-1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1.0]:\n",
        "  scale = np.sqrt(num_snps / (sumstat_samples_size * h))\n",
        "\n",
        "  inf_post_mean, inf_post_var = compute_inf_model_posterior(\n",
        "      ld_matrix_tensor, gwas_betas, scale=scale.astype(np.float32)\n",
        "  )\n",
        "  beta_init = inf_post_mean\n",
        "\n",
        "  snp_effects = (beta_init * snp_betas_se)[:, None]\n",
        "  prs = np.dot(snp_matrix, snp_effects).flatten()\n",
        "  pearson_corr = np.corrcoef(prs, pheno)[0, 1]\n",
        "\n",
        "  print(f'Pearson correlation for inf model (h={h}): {pearson_corr: 0.4f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5TeqACddb5D"
      },
      "source": [
        "### Optimization params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXQn9frP7fx1"
      },
      "outputs": [],
      "source": [
        "_KL_WEIGHT = tf.convert_to_tensor(1.0 / num_snps, dtype=tf.float32)\n",
        "_NUM_KL_APPROX_SAMPLES = 50\n",
        "\n",
        "scales_1 = np.arange(1e-5, 1.0, step=0.05)\n",
        "scale_2 = 1e-7\n",
        "mixture_probs = np.logspace(-5, -2, 5)\n",
        "\n",
        "print(f'Total number of models to fit: {len(scales_1) * len(mixture_probs)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JWPwA9k7jsa"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "In this case, we don't have any input to the model, so we create a fake input to\n",
        "bypass. Additionally, our loss function is completely encapsulated in the\n",
        "`ViLayer`, so we provide a loss that returns zero. Again this setup is useful\n",
        "if you wanted to expand functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_pC2nhxSYfy"
      },
      "outputs": [],
      "source": [
        "mock_input = tf.keras.layers.Input(shape=(1,))\n",
        "features = [[1.0]]\n",
        "labels = [[1.0]]\n",
        "mock_training_data = (\n",
        "    tf.data.Dataset.from_tensor_slices((features, labels)).cache().repeat()\n",
        ")\n",
        "results = []\n",
        "\n",
        "\n",
        "def _create_model(num_snps, scale_1, scale_2, mixture_prob, beta_init):\n",
        "  layer = ViLayer(num_snps, scale_1, scale_2, mixture_prob, beta_init)\n",
        "  return layer, tf.keras.Model(inputs=mock_input, outputs=layer(mock_input))\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def empty_context():\n",
        "  yield\n",
        "\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver) if USE_TPU else None\n",
        "# We need a context if running on TPU, otherwise just use a no-op context.\n",
        "context = strategy.scope if strategy else empty_context\n",
        "\n",
        "for scale_1 in scales_1:\n",
        "  for mixture_prob in mixture_probs:\n",
        "    print(\n",
        "        f'Fitting model scale_1={scale_1}, scale_2={scale_2}, '\n",
        "        f'mixture_prob={mixture_prob}.'\n",
        "    )\n",
        "    with context() as _:\n",
        "      tf.random.set_seed(1234)\n",
        "      layer, mdl = _create_model(\n",
        "          num_snps, scale_1, scale_2, mixture_prob, beta_init\n",
        "      )\n",
        "      opt = tf.keras.optimizers.Adam(learning_rate=BBVIPRS_LEARNING_RATE)\n",
        "      mdl.compile(\n",
        "          optimizer=opt, loss=lambda _, __: 0.00, steps_per_execution=50\n",
        "      )\n",
        "    mdl.fit(\n",
        "        mock_training_data, epochs=1, steps_per_epoch=BBVIPRS_OPTIMIZATION_STEPS\n",
        "    )\n",
        "\n",
        "    snp_effects = (layer.posterior_model(0).mean() * snp_betas_se)[:, None]\n",
        "    prs = np.dot(snp_matrix, snp_effects).flatten()\n",
        "    pearson_corr = np.corrcoef(prs, pheno)[0, 1]\n",
        "    results.append((scale_1, scale_2, mixture_prob, pearson_corr))\n",
        "    print(f'Pearson correlation : {pearson_corr: 0.4f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsElkXO8NFTe"
      },
      "source": [
        "#### Compile results\n",
        "\n",
        "The result dataframe has one row for each of the models (100 or so) fit above.\n",
        "It shows the hyperparams `scale_1`, `scale_2` and `mixture_prob` used to \n",
        "define the prior and the Pearson's correlation estimate that the model\n",
        "achieved. Note that we are computing the Pearson's correlation in the training\n",
        "sample, so this could be an overestimate. \n",
        "\n",
        "In the LDPred2 tutorial, we see that they achieved a Pearson's correlation\n",
        "of about 0.49. Given that bbviPRS tends to underperform relative to LDPred2\n",
        "in some cases, this discrepancy probably makes sense. But it is hard to say\n",
        "since we don't know much about the input data. As a result, this result is \n",
        "really just a proof of principle to illustrate how to implement and run\n",
        "bbviPRS in a way that is similar to LDPred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0RdM2ITEqMn"
      },
      "outputs": [],
      "source": [
        "full_result_df = pd.DataFrame(\n",
        "    results, columns=['scale_1', 'scale_2', 'mixture_prob', 'pearson']\n",
        ").sort_values(by='pearson', ascending=False)\n",
        "full_result_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpcq7SzKR2DB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\tscale_1\tscale_2\tmixture_prob\tpearson\n",
        "47\t0.45001\t1.000000e-07\t0.000316\t0.445777\n",
        "57\t0.55001\t1.000000e-07\t0.000316\t0.443240\n",
        "26\t0.25001\t1.000000e-07\t0.000056\t0.441404\n",
        "62\t0.60001\t1.000000e-07\t0.000316\t0.441263\n",
        "77\t0.75001\t1.000000e-07\t0.000316\t0.440966\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "gk0cqtIhRuj2",
        "xP7kxaC8xdti",
        "5eSnbsG6OHrJ",
        "A38BUoUnkmoU",
        "2VSB9JiWJ552"
      ],
      "last_runtime": {
        "build_target": "//learning/genomics/internal:genomics_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1UQeKQjjgLwvoYETlEWZelpiwegGwpVX1",
          "timestamp": 1676064066612
        }
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
